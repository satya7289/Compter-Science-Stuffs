{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Specifying which version of library is used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make tensorflow compitable with olderversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.compat.v1.disable_eager_execution()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load mnist dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist = tf.keras.datasets.mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the dataset into testing and training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "x_val = x_test[7000:]\n",
    "y_val = y_test[7000:]\n",
    "\n",
    "x_test = x_test[:7000]\n",
    "y_test = y_test[:7000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining some variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "l_mnist = 784\n",
    "l_labels = 10\n",
    "learning_rate = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define batch generator function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_generator(X, Y, batch_size):\n",
    "    indices = np.arange(len(X))\n",
    "    batch = []\n",
    "    while True:\n",
    "        np.random.shuffle(indices)\n",
    "        for i in indices:\n",
    "            batch.append(i)\n",
    "            if len(batch)==batch_size:\n",
    "                yield X[batch], Y[batch]\n",
    "                batch =[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models using 0 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for for number of weigth 1000: 99.16999986208975\n",
      "Accuracy for for number of weigth 2000: 98.0099998600781\n"
     ]
    }
   ],
   "source": [
    "img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_mnist])\n",
    "ans = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_labels])\n",
    "\n",
    "# Weight and baise\n",
    "weight = tf.Variable(tf.random.normal([l_mnist,l_labels], stddev=0.1))\n",
    "baise = tf.Variable(tf.random.normal([l_labels], stddev=0.1))\n",
    "\n",
    "# calculating probability\n",
    "probs = tf.nn.softmax(tf.matmul(img, weight) + baise)\n",
    "\n",
    "# calculating entrophy\n",
    "xEntrophy = tf.reduce_mean(-tf.reduce_sum(ans*tf.math.log(probs)))\n",
    "\n",
    "# build train model using GradientDescentOptimizer\n",
    "train = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(xEntrophy)\n",
    "\n",
    "# Finding Accuracy\n",
    "numCorrect = tf.equal(tf.argmax(probs,1), tf.argmax(ans,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(numCorrect, tf.float32))\n",
    "\n",
    "# Create Session for tensorflow\n",
    "session = tf.compat.v1.Session()\n",
    "session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# training start\n",
    "train_generator = batch_generator(x_train, y_train, batch_size)\n",
    "\n",
    "weigth = [1000, 2000]\n",
    "for w in weigth:\n",
    "    num_of_weigth_change = w\n",
    "    for i in range(num_of_weigth_change):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size), ys] = 1\n",
    "        session.run(train, feed_dict={img:xsn, ans:ysn})\n",
    "\n",
    "    sumAcc = 0\n",
    "    for i in range(1000):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size),ys] = 1\n",
    "        sumAcc += session.run(accuracy, {img:xsn, ans:ysn})\n",
    "    print(\"Accuracy for for number of weigth {}: {}\".format(num_of_weigth_change, sumAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models using 1 layer using relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for for number of weigth 1000: 98.16000000201166\n",
      "Accuracy for for number of weigth 2000: 98.77999989688396\n"
     ]
    }
   ],
   "source": [
    "img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_mnist])\n",
    "ans = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_labels])\n",
    "\n",
    "nFirstLayer = 20\n",
    "# U, bU are weigth and baise for vector leading input to  first hidden layer\n",
    "U = tf.Variable(tf.random.normal([l_mnist, nFirstLayer], stddev=0.1))\n",
    "bU = tf.Variable(tf.random.normal([nFirstLayer], stddev=0.1))\n",
    "\n",
    "# V, bV are weigth and baise for vector leading hidden layer to output layer\n",
    "V = tf.Variable(tf.random.normal([nFirstLayer, l_labels], stddev=0.1))\n",
    "bV = tf.Variable(tf.random.normal([l_labels], stddev=0.1))\n",
    "\n",
    "# Affine tranformation for first layer and passed through relu function\n",
    "L1Output = tf.matmul(img, U) + bU\n",
    "L1Output = tf.nn.relu(L1Output)\n",
    "\n",
    "# Calculating probability by using softmax function\n",
    "probs = tf.nn.softmax(tf.matmul(L1Output, V) + bV)\n",
    "xEntrophy = tf.reduce_mean(-tf.reduce_sum(ans*tf.math.log(probs)))\n",
    "\n",
    "# build train model using GradientDescentOptimizer\n",
    "train = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(xEntrophy)\n",
    "\n",
    "# Finding Accuracy\n",
    "numCorrect = tf.equal(tf.argmax(probs,1), tf.argmax(ans,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(numCorrect, tf.float32))\n",
    "\n",
    "# Create Session for tensorflow\n",
    "session = tf.compat.v1.Session()\n",
    "session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "\n",
    "# training start\n",
    "train_generator = batch_generator(x_train, y_train, batch_size)\n",
    "\n",
    "weigth = [1000, 2000]\n",
    "for w in weigth:\n",
    "    epos = w\n",
    "    for i in range(epos):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size), ys] = 1\n",
    "        session.run(train, feed_dict={img:xsn, ans:ysn})\n",
    "\n",
    "    sumAcc = 0\n",
    "    for i in range(1000):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size),ys] = 1\n",
    "        sumAcc += session.run(accuracy, {img:xsn, ans:ysn})\n",
    "    print(\"Accuracy for for number of weigth {}: {}\".format(epos, sumAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build models using 1 layer using signmod function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for for number of weigth 1000: 98.61999990046024\n",
      "Accuracy for for number of weigth 2000: 99.20999990403652\n"
     ]
    }
   ],
   "source": [
    "img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_mnist])\n",
    "ans = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_labels])\n",
    "\n",
    "nFirstLayer = 20\n",
    "# U, bU are weigth and baise for vector leading input to  first hidden layer\n",
    "U = tf.Variable(tf.random.normal([l_mnist, nFirstLayer], stddev=0.1))\n",
    "bU = tf.Variable(tf.random.normal([nFirstLayer], stddev=0.1))\n",
    "\n",
    "# V, bV are weigth and baise for vector leading hidden layer to output layer\n",
    "V = tf.Variable(tf.random.normal([nFirstLayer, l_labels], stddev=0.1))\n",
    "bV = tf.Variable(tf.random.normal([l_labels], stddev=0.1))\n",
    "\n",
    "# Affine tranformation for first layer and passed through relu function\n",
    "L1Output = tf.matmul(img, U) + bU\n",
    "L1Output = tf.nn.sigmoid(L1Output)\n",
    "\n",
    "# Calculating probability by using softmax function\n",
    "probs = tf.nn.softmax(tf.matmul(L1Output, V) + bV)\n",
    "xEntrophy = tf.reduce_mean(-tf.reduce_sum(ans*tf.math.log(probs)))\n",
    "\n",
    "# build train model using GradientDescentOptimizer\n",
    "train = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(xEntrophy)\n",
    "\n",
    "# Finding Accuracy\n",
    "numCorrect = tf.equal(tf.argmax(probs,1), tf.argmax(ans,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(numCorrect, tf.float32))\n",
    "\n",
    "# Create Session for tensorflow\n",
    "session = tf.compat.v1.Session()\n",
    "session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "\n",
    "# training start\n",
    "train_generator = batch_generator(x_train, y_train, batch_size)\n",
    "\n",
    "weigth = [1000, 2000]\n",
    "for w in weigth:\n",
    "    epos = w\n",
    "    for i in range(epos):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size), ys] = 1\n",
    "        session.run(train, feed_dict={img:xsn, ans:ysn})\n",
    "\n",
    "    sumAcc = 0\n",
    "    for i in range(1000):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size),ys] = 1\n",
    "        sumAcc += session.run(accuracy, {img:xsn, ans:ysn})\n",
    "    print(\"Accuracy for for number of weigth {}: {}\".format(epos, sumAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model using 2 layer using Relu function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for for number of weigth 1000: 98.59999986179173\n",
      "Accuracy for for number of weigth 2000: 99.31999995745718\n"
     ]
    }
   ],
   "source": [
    "img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_mnist])\n",
    "ans = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_labels])\n",
    "\n",
    "nFirstLayer = 20\n",
    "# U, bU are weigth and baise for vector leading input to  first hidden layer\n",
    "U = tf.Variable(tf.random.normal([l_mnist, nFirstLayer], stddev=0.1))\n",
    "bU = tf.Variable(tf.random.normal([nFirstLayer], stddev=0.1))\n",
    "\n",
    "# U2, bU2 are weigth and baise for vector leading first hidden layer to second hidden layer\n",
    "U2 = tf.Variable(tf.random.normal([l_mnist, nFirstLayer], stddev=0.1))\n",
    "bU2 = tf.Variable(tf.random.normal([nFirstLayer], stddev=0.1))\n",
    "\n",
    "# V, bV are weigth and baise for vector leading hidden layer to output layer\n",
    "V = tf.Variable(tf.random.normal([nFirstLayer, l_labels], stddev=0.1))\n",
    "bV = tf.Variable(tf.random.normal([l_labels], stddev=0.1))\n",
    "\n",
    "# Affine tranformation for first layer and passed through relu function\n",
    "L1Output = tf.matmul(img, U) + bU\n",
    "L1Output = tf.nn.relu(L1Output)\n",
    "\n",
    "# Affine tranformation for second layer and passed through relu function\n",
    "L2Output = tf.matmul(img, U2) + bU2\n",
    "L2Output = tf.nn.relu(L2Output)\n",
    "\n",
    "# Calculating probability by using softmax function\n",
    "probs = tf.nn.softmax(tf.matmul(L2Output, V) + bV)\n",
    "\n",
    "# Calculating Entrophy \n",
    "xEntrophy = tf.reduce_mean(-tf.reduce_sum(ans*tf.math.log(probs)))\n",
    "\n",
    "# build train model using GradientDescentOptimizer\n",
    "train = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(xEntrophy)\n",
    "\n",
    "# Finding Accuracy\n",
    "numCorrect = tf.equal(tf.argmax(probs,1), tf.argmax(ans,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(numCorrect, tf.float32))\n",
    "\n",
    "# Create Session for tensorflow\n",
    "session = tf.compat.v1.Session()\n",
    "session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# start training\n",
    "train_generator = batch_generator(x_train, y_train, batch_size)\n",
    "\n",
    "weigth = [1000, 2000]\n",
    "for w in weigth:\n",
    "    epos = w\n",
    "    for i in range(epos):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size), ys] = 1\n",
    "        session.run(train, feed_dict={img:xsn, ans:ysn})\n",
    "\n",
    "    sumAcc = 0\n",
    "    for i in range(1000):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size),ys] = 1\n",
    "        sumAcc += session.run(accuracy, {img:xsn, ans:ysn})\n",
    "    print(\"Accuracy for for number of weigth {}: {}\".format(epos, sumAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Model using 2 layer using Sigmod function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy for for number of weigth 1000: 99.51999997720122\n",
      "Accuracy for for number of weigth 2000: 98.65999989770353\n"
     ]
    }
   ],
   "source": [
    "img = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_mnist])\n",
    "ans = tf.compat.v1.placeholder(dtype=tf.float32, shape=[batch_size, l_labels])\n",
    "\n",
    "nFirstLayer = 20\n",
    "# U, bU are weigth and baise for vector leading input to  first hidden layer\n",
    "U = tf.Variable(tf.random.normal([l_mnist, nFirstLayer], stddev=0.1))\n",
    "bU = tf.Variable(tf.random.normal([nFirstLayer], stddev=0.1))\n",
    "\n",
    "# U2, bU2 are weigth and baise for vector leading first hidden layer to second hidden layer\n",
    "U2 = tf.Variable(tf.random.normal([l_mnist, nFirstLayer], stddev=0.1))\n",
    "bU2 = tf.Variable(tf.random.normal([nFirstLayer], stddev=0.1))\n",
    "\n",
    "# V, bV are weigth and baise for vector leading hidden layer to output layer\n",
    "V = tf.Variable(tf.random.normal([nFirstLayer, l_labels], stddev=0.1))\n",
    "bV = tf.Variable(tf.random.normal([l_labels], stddev=0.1))\n",
    "\n",
    "# Affine tranformation for first layer and passed through relu function\n",
    "L1Output = tf.matmul(img, U) + bU\n",
    "L1Output = tf.nn.sigmoid(L1Output)\n",
    "\n",
    "# Affine tranformation for second layer and passed through relu function\n",
    "L2Output = tf.matmul(img, U2) + bU2\n",
    "L2Output = tf.nn.sigmoid(L2Output)\n",
    "\n",
    "# Calculating probability by using softmax function\n",
    "probs = tf.nn.softmax(tf.matmul(L2Output, V) + bV)\n",
    "\n",
    "# Calculating Entrophy \n",
    "xEntrophy = tf.reduce_mean(-tf.reduce_sum(ans*tf.math.log(probs)))\n",
    "\n",
    "# build train model using GradientDescentOptimizer\n",
    "train = tf.compat.v1.train.GradientDescentOptimizer(learning_rate).minimize(xEntrophy)\n",
    "\n",
    "# Finding Accuracy\n",
    "numCorrect = tf.equal(tf.argmax(probs,1), tf.argmax(ans,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(numCorrect, tf.float32))\n",
    "\n",
    "# Create Session for tensorflow\n",
    "session = tf.compat.v1.Session()\n",
    "session.run(tf.compat.v1.global_variables_initializer())\n",
    "\n",
    "# Start training\n",
    "train_generator = batch_generator(x_train, y_train, batch_size)\n",
    "\n",
    "weigth = [1000, 2000]\n",
    "for w in weigth:\n",
    "    epos = w\n",
    "    for i in range(epos):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size), ys] = 1\n",
    "        session.run(train, feed_dict={img:xsn, ans:ysn})\n",
    "\n",
    "    sumAcc = 0\n",
    "    for i in range(1000):\n",
    "        xs, ys = next(train_generator)\n",
    "        xsn = xs.reshape(batch_size,l_mnist)\n",
    "        ysn = np.zeros((ys.size, ys.max()+1))\n",
    "        ysn[np.arange(ys.size),ys] = 1\n",
    "        sumAcc += session.run(accuracy, {img:xsn, ans:ysn})\n",
    "    print(\"Accuracy for for number of weigth {}: {}\".format(epos, sumAcc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using Keras Liberary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using 1 layer relu activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.9948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.28590078176930545,\n",
       "  0.11627211503994962,\n",
       "  0.08070516010591139,\n",
       "  0.05817798526026308,\n",
       "  0.04602849234283591,\n",
       "  0.037039140319490495,\n",
       "  0.028143381598832395,\n",
       "  0.0241859797766665,\n",
       "  0.020747485711229578,\n",
       "  0.01571817357544205],\n",
       " 'accuracy': [0.9179,\n",
       "  0.96533334,\n",
       "  0.97515,\n",
       "  0.9820833,\n",
       "  0.98546666,\n",
       "  0.98825,\n",
       "  0.9909,\n",
       "  0.99225,\n",
       "  0.99325,\n",
       "  0.99486667],\n",
       " 'val_loss': [0.09726542321344217,\n",
       "  0.07459922333558401,\n",
       "  0.05586053871860107,\n",
       "  0.051744054754575095,\n",
       "  0.049084736655155815,\n",
       "  0.0559089469568183,\n",
       "  0.04732186727924272,\n",
       "  0.053257337453154224,\n",
       "  0.043421285166793194,\n",
       "  0.05249617339771551],\n",
       " 'val_accuracy': [0.96933335,\n",
       "  0.97933334,\n",
       "  0.98333335,\n",
       "  0.98733336,\n",
       "  0.9856667,\n",
       "  0.9813333,\n",
       "  0.989,\n",
       "  0.9853333,\n",
       "  0.98933333,\n",
       "  0.988]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=100, validation_data=(x_val, y_val), verbose=0)\n",
    "test_loss, test_acc = model.evaluate(x_train,  y_train, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model using 1 layer sigmoid activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test accuracy: 0.99023336\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'loss': [0.6279800433417161,\n",
       "  0.2314765346298615,\n",
       "  0.17229543319592874,\n",
       "  0.13659964537558456,\n",
       "  0.11114081292413175,\n",
       "  0.09136032730651399,\n",
       "  0.07554899915431937,\n",
       "  0.06414874405134469,\n",
       "  0.053563755290039504,\n",
       "  0.0456554550724104],\n",
       " 'accuracy': [0.8425,\n",
       "  0.93275,\n",
       "  0.94906664,\n",
       "  0.9600833,\n",
       "  0.96753335,\n",
       "  0.9730167,\n",
       "  0.97781664,\n",
       "  0.98116666,\n",
       "  0.9841833,\n",
       "  0.9867667],\n",
       " 'val_loss': [0.19015120243032774,\n",
       "  0.13036363509794077,\n",
       "  0.10886791038016479,\n",
       "  0.0919844876974821,\n",
       "  0.0726894499734044,\n",
       "  0.06757935583591461,\n",
       "  0.06202606704706947,\n",
       "  0.05732048371185859,\n",
       "  0.05276733363668124,\n",
       "  0.04905499958743652],\n",
       " 'val_accuracy': [0.94666666,\n",
       "  0.96166664,\n",
       "  0.971,\n",
       "  0.97466666,\n",
       "  0.9773333,\n",
       "  0.98066664,\n",
       "  0.981,\n",
       "  0.982,\n",
       "  0.98433334,\n",
       "  0.986]}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(28, 28)),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(128, activation='sigmoid'),\n",
    "    tf.keras.layers.Dense(10)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam',\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'])\n",
    "history = model.fit(x_train, y_train, epochs=10, batch_size=100, validation_data=(x_val, y_val), verbose=0)\n",
    "test_loss, test_acc = model.evaluate(x_train,  y_train, verbose=2)\n",
    "print('\\nTest accuracy:', test_acc)\n",
    "history.history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
